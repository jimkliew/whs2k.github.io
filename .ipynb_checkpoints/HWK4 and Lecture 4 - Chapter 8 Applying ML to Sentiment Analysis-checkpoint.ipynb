{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download data from site -- http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "# p.234\n",
    "#import pyprind # may need to do >sudo easy_install pip, then >pip install pyprind --user\n",
    "import pandas as pd\n",
    "import os\n",
    "import pyprind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/whs/Documents/Data Journalism/Congressional Tweets/aclImdb/\n"
     ]
    }
   ],
   "source": [
    "pwd = os.getcwd()\n",
    "print(pwd+'/aclImdb/')\n",
    "file = os.listdir(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49999, 2)\n"
     ]
    }
   ],
   "source": [
    "#################\n",
    "# Start here    #\n",
    "#################\n",
    "#import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "pwd = os.getcwd()\n",
    "\n",
    "df = pd.read_csv( 'movie_data.csv', encoding='utf-8')\n",
    "df.columns = ['review', 'sentiment']\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pwd+'/movie_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  I recently bought the DVD, forgetting just how...          0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 5, 'sun': 3, 'is': 1, 'shining': 2, 'weather': 6, 'sweet': 4, 'and': 0}\n",
      "[[0 1 1 1 0 1 0]\n",
      " [0 1 0 0 1 1 1]\n",
      " [1 2 1 1 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "# Bag of Word model\n",
    "# 1. create a vocabulary of unique tokens (or words)\n",
    "# 2. construct a feature vector for each document, features store count\n",
    "#    of words per document\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer() #Instanstiate the count array\n",
    "\n",
    "docs = np.array(['The sun is shining', \n",
    "                 'The weather is sweet',\n",
    "                 'The sun is shining and the weather is sweet'])\n",
    "\n",
    "bag = count.fit_transform(docs)\n",
    "\n",
    "print(count.vocabulary_)\n",
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf(t,d) - raw term frequencies (t: term, d: nos times term t appears in doc d)\n",
    "# tf-idf(t,d) - term frequency inverse document frequency\n",
    "# tf-idf = tf(t,d) * idf(t,x)  = tf(t,d) * log( [1+nd]/[1+df(d,t)] ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.    0.43  0.56  0.56  0.    0.43  0.  ]\n",
      " [ 0.    0.43  0.    0.    0.56  0.43  0.56]\n",
      " [ 0.4   0.48  0.31  0.31  0.31  0.48  0.31]]\n"
     ]
    }
   ],
   "source": [
    "# TfidTransformer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer() #Instantiate Term Frequency invers\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray()) #How much did the term appear in other documents?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# so if the term \"the\" shows up lots of times, does that mean it's important?\n",
    "# how can we make terms that shows up lots of times across documents, less important\n",
    "# let's normalize by the times these terms show up across documents.\n",
    "\n",
    "# employ : [nos of docs containing term \"the\" ]/[total nos of documents]\n",
    "\n",
    "# if term appear often, give it less emphasis\n",
    "\n",
    "# tf-idf(t,d) = tf(t,2)*(idf(t,d)+1)\n",
    "# with idf(t,d) = log ([1+total nos of docs]/[1+nos of docs containing term t])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reg functions...to get rid of HTML Tags and emoticons\n",
    "\n",
    "import re\n",
    "def preprocessor(text): \n",
    "# find '<' then anything not '>' [^>], [^>]* 0 or more prefix, then close with '>'    \n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text) \n",
    "    # eyes[:,;,=], optional nose [-], and mouth[),(,D,P)]\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is seven title brazil not available;) :)\n"
     ]
    }
   ],
   "source": [
    "tmp = 'is ;) :) seven.<br /><br />Title (Brazil): Not Available'\n",
    "\n",
    "print(preprocessor(tmp))\n",
    "#print(preprocessor('</a>This :) is :( a test :-)!' ))\n",
    "#print(re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', '</a>This :) is :( a test :-)!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(preprocessor) #use the apply method and send in the preprocessor function (applys the function to each row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49999, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>i don t even know where to begin on this one i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>richard tyler is a little boy who is scared of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>i waited long to watch this movie also because...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "49996  i don t even know where to begin on this one i...          0\n",
       "49997  richard tyler is a little boy who is scared of...          0\n",
       "49998  i waited long to watch this movie also because...          1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# p.242 Processing documents into tokens\n",
    "# split the sentence/corpora into individual elements\n",
    "def tokenizer(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['running', 'like', 'running', 'and', 'thus', 'they', 'run']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('running like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word stemming, tranforming word into their root form\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_porter('running like running and thus they run')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/whs/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'run', 'lot']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')  # stop words have little meaning eg. a, is, and, has, etc. \n",
    "[w for w in tokenizer_porter('a runner likes running and runs a lot') \n",
    " if w not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2501,)\n"
     ]
    }
   ],
   "source": [
    "# pg. 244\n",
    "# Training a Logistic Regression model for document classification\n",
    "# (X,y)\n",
    "#X_train = df.loc[:25000, 'review'].values\n",
    "#y_train = df.loc[:25000, 'sentiment'].values\n",
    "\n",
    "#X_test  = df.loc[25000:, 'review'].values\n",
    "#y_test  = df.loc[25000:, 'sentiment'].values\n",
    "\n",
    "X_train = df.loc[:2500, 'review'].values\n",
    "y_train = df.loc[:2500, 'sentiment'].values\n",
    "\n",
    "X_test  = df.loc[2500:5000, 'review'].values\n",
    "y_test  = df.loc[2500:5000, 'sentiment'].values\n",
    "\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents = None, \n",
    "                       lowercase = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "              {'vect__ngram_range':[(1,1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer], #, tokenizer_porter],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [100]}, # 0.1, 1.0, 10.0, 100.0]},\n",
    "              \n",
    "              {'vect__ngram_range': [(1,1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer], #, tokenizer_porter],\n",
    "               'vect__use_idf':[False],\n",
    "               'vect__norm':[None],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C':[100]} #[0.1, 1.0,10.0,100.0]}\n",
    "                ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_tfidf = Pipeline([ ('vect', tfidf) ,\n",
    "                      ('clf',  LogisticRegression(random_state=0))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs_lr_tfidf = GridSearchCV( lr_tfidf, param_grid, #sends each subset to a different core\n",
    "                          scoring = 'accuracy',\n",
    "                          cv = 5, verbose = 1,\n",
    "                          n_jobs = -1) # n_jobs -1 uses all computer cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2501,) (2501,)\n",
      "(2501,) (2501,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   13.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...nalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid=[{'vect__ngram_range': [(1, 1)], 'vect__stop_words': [['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', '...8>], 'vect__use_idf': [False], 'vect__norm': [None], 'clf__penalty': ['l1', 'l2'], 'clf__C': [100]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr_tfidf.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Best parameter set: {'clf__C': 100, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x10f173bf8>}\n"
     ]
    }
   ],
   "source": [
    "print('The Best parameter set: %s' % gs_lr_tfidf.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy: 0.855\n",
      "Test Accuracy: 0.848\n"
     ]
    }
   ],
   "source": [
    "print('CV Accuracy: %.3f'\n",
    "     % gs_lr_tfidf.best_score_)\n",
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print('Test Accuracy: %.3f' % clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>879550061075673088</td>\n",
       "      <td>https://www.twitter.com/GarrettforVA/statuses/...</td>\n",
       "      <td>GarrettforVA</td>\n",
       "      <td>Twitter Web Client</td>\n",
       "      <td>CNN Employees Resign Over Retracted Story on R...</td>\n",
       "      <td>2017-06-27T00:00:50-04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>879559193283448832</td>\n",
       "      <td>https://www.twitter.com/RepSwalwell/statuses/8...</td>\n",
       "      <td>RepSwalwell</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Ribbeting read. I've been toad @ScottHerhold w...</td>\n",
       "      <td>2017-06-27T00:37:07-04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>879557174673932288</td>\n",
       "      <td>https://www.twitter.com/senorrinhatch/statuses...</td>\n",
       "      <td>senorrinhatch</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Very valuable point--&amp;gt; https://twitter.com/...</td>\n",
       "      <td>2017-06-27T00:29:06-04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>879556293303971840</td>\n",
       "      <td>https://www.twitter.com/benwikler/statuses/879...</td>\n",
       "      <td>SenBookerOffice</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @benwikler 1. Tonight, something kinda magi...</td>\n",
       "      <td>2017-06-27T00:25:36-04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>879553333316517888</td>\n",
       "      <td>https://www.twitter.com/SenatorDurbin/statuses...</td>\n",
       "      <td>SenatorDurbin</td>\n",
       "      <td>Twitter Web Client</td>\n",
       "      <td>Joined @CoryBooker, @repjohnlewis &amp;amp; Dem co...</td>\n",
       "      <td>2017-06-27T00:13:50-04:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                                               link  \\\n",
       "0  879550061075673088  https://www.twitter.com/GarrettforVA/statuses/...   \n",
       "1  879559193283448832  https://www.twitter.com/RepSwalwell/statuses/8...   \n",
       "2  879557174673932288  https://www.twitter.com/senorrinhatch/statuses...   \n",
       "3  879556293303971840  https://www.twitter.com/benwikler/statuses/879...   \n",
       "4  879553333316517888  https://www.twitter.com/SenatorDurbin/statuses...   \n",
       "\n",
       "       screen_name              source  \\\n",
       "0     GarrettforVA  Twitter Web Client   \n",
       "1      RepSwalwell  Twitter for iPhone   \n",
       "2    senorrinhatch  Twitter for iPhone   \n",
       "3  SenBookerOffice  Twitter for iPhone   \n",
       "4    SenatorDurbin  Twitter Web Client   \n",
       "\n",
       "                                                text  \\\n",
       "0  CNN Employees Resign Over Retracted Story on R...   \n",
       "1  Ribbeting read. I've been toad @ScottHerhold w...   \n",
       "2  Very valuable point--&gt; https://twitter.com/...   \n",
       "3  RT @benwikler 1. Tonight, something kinda magi...   \n",
       "4  Joined @CoryBooker, @repjohnlewis &amp; Dem co...   \n",
       "\n",
       "                        time  \n",
       "0  2017-06-27T00:00:50-04:00  \n",
       "1  2017-06-27T00:37:07-04:00  \n",
       "2  2017-06-27T00:29:06-04:00  \n",
       "3  2017-06-27T00:25:36-04:00  \n",
       "4  2017-06-27T00:13:50-04:00  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_20170627.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shiz getting real - Tweet Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2017-06-29'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime as dt\n",
    "\n",
    "#Ceate a vaiarble: todays_tweets = \n",
    "#today=dt.datetime.today().strftime(\"%m/%d/%Y\")\n",
    "date = dt.date.today().strftime(\"%Y-%m-%d\")\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.48583523282318464,\n",
       " 0.47185493010955798,\n",
       " 0.48214285714285715,\n",
       " 0.59193954659949621,\n",
       " 0.49723756906077349,\n",
       " 0.49258908861557871]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####Predict Time \n",
    "\n",
    "#Ceate a vaiarble: todays_tweets = \n",
    "today=dt.datetime.today().strftime(\"%m/%d/%Y\")\n",
    "\n",
    "\n",
    "### Let's import our data\n",
    "df_20170627 = pd.read_json('2017-06-27.json')\n",
    "df_20170626 = pd.read_json('2017-06-26.json')\n",
    "df_20170625 = pd.read_json('2017-06-25.json')\n",
    "df_20170624 = pd.read_json('2017-06-24.json')\n",
    "df_20170623 = pd.read_json('2017-06-23.json')\n",
    "df_20170622 = pd.read_json('2017-06-22.json')\n",
    "\n",
    "#clf.predict(X_test[:10])\n",
    "#X_test[:10]\n",
    "predict_20170627=np.mean(clf.predict(df_20170627['text']))\n",
    "predict_20170626=np.mean(clf.predict(df_20170626['text']))\n",
    "predict_20170625=np.mean(clf.predict(df_20170625['text']))\n",
    "predict_20170624=np.mean(clf.predict(df_20170624['text']))\n",
    "predict_20170623=np.mean(clf.predict(df_20170623['text']))\n",
    "predict_20170622=np.mean(clf.predict(df_20170622['text']))\n",
    "\n",
    "df_plot=[predict_20170627, predict_20170626,\n",
    "         predict_20170625,predict_20170624,\n",
    "         predict_20170623,predict_20170622]\n",
    "\n",
    "dates = pd.date_range('20170622', periods=6)\n",
    "\n",
    "\n",
    "dates\n",
    "df_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_20170627' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-88e65da3d10a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfrowney\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Frowney.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mpredict_20170627\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mfacePic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmiley\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predict_20170627' is not defined"
     ]
    }
   ],
   "source": [
    "#set pictue\n",
    "from IPython.display import Image\n",
    "smiley=Image(\"Smiley.png\")\n",
    "frowney=Image(\"Frowney.png\")\n",
    "\n",
    "if predict_20170627 > .5:\n",
    "    facePic = smiley \n",
    "else:\n",
    "    facePic = frowney\n",
    "    \n",
    "facePic\n",
    "\n",
    "#https://github.com/whs2k/whs2k.github.io/blob/master/Frowney.png\n",
    "#import urllib\n",
    "#url = 'https://github.com/whs2k/whs2k.github.io/blob/master/Frowney.png?raw=true'\n",
    "#image = urllib.urlopen(url).read()\n",
    "#outfile = open('chart01.png','wb')\n",
    "#outfile.write(image)\n",
    "#outfile.close()\n",
    "\n",
    "import urllib.request\n",
    "with urllib.request.urlopen('https://github.com/whs2k/whs2k.github.io/blob/master/Frowney.png?raw=true') as url:\n",
    "    facePic = url.read()\n",
    "#I'm guessing this would output the html source code?\n",
    "#print(s)\n",
    "outfile = open('facePic.png','wb')\n",
    "outfile.write(facePic)\n",
    "outfile.close()\n",
    "facePic\n",
    "\n",
    "\n",
    "#Export Image\n",
    "#import urllib.request\n",
    "#urllib.request.urlretrieve('http://chart.apis.google.com/...', 'outfile.png')\n",
    "\n",
    "#outfile = open('chart01.png','wb')\n",
    "#outfile.write(facePic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X-Axis - Days\n",
    "#max_year=df['fiscal_year'].max()\n",
    "#min_year=df['fiscal_year'].min()\n",
    "#years=np.linspace(min_year, max_year, (max_year-min_year+1))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "thfont = {'fontname':'Tahoma'}\n",
    "\n",
    "#plt.plot(years, WS_PER,'r', label='WS Estimates')\n",
    "\n",
    "\n",
    "plt.plot(dates, df_plot,'b', label='EMZ Estimates')\n",
    "plt.xlabel('Date',**thfont)\n",
    "plt.ylabel('Sentiment',**thfont)\n",
    "\n",
    "\n",
    "plt.savefig('whs2k.github.io/todaysMood.png')\n",
    "\n",
    "#todaysMood=plt.show()\n",
    "#todaysMood.savefig()\n",
    "#savefig('todaysMood.png',bbox_inches='tight')\n",
    "#pylab.savefig('todaysMood.png')savefig('foo.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-108-5224029f8002>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-108-5224029f8002>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    git add #(ads a file)\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#IMpotant Git Commands\n",
    "#git add "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from threading import Timer\n",
    "\n",
    "x=datetime.today()\n",
    "y=x.replace(day=x.day+1, hour=1, minute=0, second=0, microsecond=0)\n",
    "delta_t=y-x\n",
    "\n",
    "secs=delta_t.seconds+1\n",
    "\n",
    "def hello_world():\n",
    "    print (\"hello world\")\n",
    "    #...\n",
    "\n",
    "t = Timer(secs, hello_world)\n",
    "t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# p.246\n",
    "# Working with bigger data -- online algos and out-of-core learning\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):  # converts reviews to lower case, take out non-words, and put emoticons at the end\n",
    "    text = re.sub('<[^>]*>', '', text) #\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "def stream_docs(path): #brings in the reviews\n",
    "    with open(path, 'r', encoding='utf-8')  as csv:\n",
    "        next(csv) # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2]) \n",
    "            yield text, label \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer #New trick with hashing vectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "vect = HashingVectorizer(decode_error = 'ignore',\n",
    "                         n_features = 2**21,\n",
    "                         preprocessor = None,\n",
    "                         tokenizer = tokenizer)\n",
    "\n",
    "clf = SGDClassifier(loss='log', random_state = 1, n_iter = 1)\n",
    "\n",
    "doc_stream = stream_docs(path ='/Users/whs/Documents/Grad/Machine Learning/Class 4/movie_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "#import pyprind\n",
    "classes = np.array([0, 1])\n",
    "print(classes)\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes = classes) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.882\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, size = 500)\n",
    "X_test = vect.transform(X_test)\n",
    "print('Accuracy: %.3f' % clf.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Trailing data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-c2dc8a070399>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_20170628\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2017-06-28.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Applications/anaconda/lib/python3.6/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines)\u001b[0m\n\u001b[1;32m    352\u001b[0m         obj = FrameParser(json, orient, dtype, convert_axes, convert_dates,\n\u001b[1;32m    353\u001b[0m                           \u001b[0mkeep_default_dates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m                           date_unit).parse()\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'series'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.6/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.6/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m--> 639\u001b[0;31m                 loads(json, precise_float=self.precise_float), dtype=None)\n\u001b[0m\u001b[1;32m    640\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             decoded = dict((str(k), v)\n",
      "\u001b[0;31mValueError\u001b[0m: Trailing data"
     ]
    }
   ],
   "source": [
    "df_20170628 = pd.read_json('2017-06-28.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Homework 4 - \"mini-Kaggle competition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HWK 4 - \"In-class IMDB Kaggle competition\" \n",
    "\n",
    "# 1. Use the whole data per above for your train/test split \n",
    "#    (note you may use the whole data set too!)\n",
    "#\n",
    "#    X_train = df.loc[:25000, 'review'].values\n",
    "#    y_train = df.loc[:25000, 'sentiment'].values\n",
    "#    X_test  = df.loc[25000:, 'review'].values\n",
    "#    y_test  = df.loc[25000:, 'sentiment'].values\n",
    "\n",
    "# 2. Employ any technique learnt in class or outside of class to arrive at \n",
    "#    the best ML engine model for predicting movie sentiment base on reviews\n",
    "#\n",
    "#    Do 5-fold cross-validation and report your best model's train/test accuracy, f1-score, and auc\n",
    "#    The winner will be the ML algo model that has the highest accuracy!\n",
    "#    Just as in the regular Kaggle competition combining models will be acceptable (collusion is ok!). \n",
    "#    However, please turn in your notebook individually to Blackbaord. Let the games begin!  \n",
    "\n",
    "#Try on/off\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
