{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download data from site -- http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "# p.234\n",
    "#import pyprind # may need to do >sudo easy_install pip, then >pip install pyprind --user\n",
    "import pandas as pd\n",
    "import os\n",
    "import pyprind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/whs/Documents/Data Journalism/Congressional Tweets/whs2k.github.io/aclImdb/\n"
     ]
    }
   ],
   "source": [
    "pwd = os.getcwd()\n",
    "print(pwd+'/aclImdb/')\n",
    "file = os.listdir(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49999, 2)\n"
     ]
    }
   ],
   "source": [
    "#################\n",
    "# Start here    #\n",
    "#################\n",
    "#import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "pwd = os.getcwd()\n",
    "\n",
    "df = pd.read_csv( 'movie_data.csv', encoding='utf-8')\n",
    "df.columns = ['review', 'sentiment']\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/whs/Documents/Data Journalism/Congressional Tweets/whs2k.github.io/movie_data.csv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd+'/movie_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  I recently bought the DVD, forgetting just how...          0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 5, 'sun': 3, 'is': 1, 'shining': 2, 'weather': 6, 'sweet': 4, 'and': 0}\n",
      "[[0 1 1 1 0 1 0]\n",
      " [0 1 0 0 1 1 1]\n",
      " [1 2 1 1 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "# Bag of Word model\n",
    "# 1. create a vocabulary of unique tokens (or words)\n",
    "# 2. construct a feature vector for each document, features store count\n",
    "#    of words per document\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer() #Instanstiate the count array\n",
    "\n",
    "docs = np.array(['The sun is shining', \n",
    "                 'The weather is sweet',\n",
    "                 'The sun is shining and the weather is sweet'])\n",
    "\n",
    "bag = count.fit_transform(docs)\n",
    "\n",
    "print(count.vocabulary_)\n",
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf(t,d) - raw term frequencies (t: term, d: nos times term t appears in doc d)\n",
    "# tf-idf(t,d) - term frequency inverse document frequency\n",
    "# tf-idf = tf(t,d) * idf(t,x)  = tf(t,d) * log( [1+nd]/[1+df(d,t)] ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.    0.43  0.56  0.56  0.    0.43  0.  ]\n",
      " [ 0.    0.43  0.    0.    0.56  0.43  0.56]\n",
      " [ 0.4   0.48  0.31  0.31  0.31  0.48  0.31]]\n"
     ]
    }
   ],
   "source": [
    "# TfidTransformer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer() #Instantiate Term Frequency invers\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray()) #How much did the term appear in other documents?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# so if the term \"the\" shows up lots of times, does that mean it's important?\n",
    "# how can we make terms that shows up lots of times across documents, less important\n",
    "# let's normalize by the times these terms show up across documents.\n",
    "\n",
    "# employ : [nos of docs containing term \"the\" ]/[total nos of documents]\n",
    "\n",
    "# if term appear often, give it less emphasis\n",
    "\n",
    "# tf-idf(t,d) = tf(t,2)*(idf(t,d)+1)\n",
    "# with idf(t,d) = log ([1+total nos of docs]/[1+nos of docs containing term t])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reg functions...to get rid of HTML Tags and emoticons\n",
    "\n",
    "import re\n",
    "def preprocessor(text): \n",
    "# find '<' then anything not '>' [^>], [^>]* 0 or more prefix, then close with '>'    \n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text) \n",
    "    # eyes[:,;,=], optional nose [-], and mouth[),(,D,P)]\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is seven title brazil not available;) :)\n"
     ]
    }
   ],
   "source": [
    "tmp = 'is ;) :) seven.<br /><br />Title (Brazil): Not Available'\n",
    "\n",
    "print(preprocessor(tmp))\n",
    "#print(preprocessor('</a>This :) is :( a test :-)!' ))\n",
    "#print(re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', '</a>This :) is :( a test :-)!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(preprocessor) #use the apply method and send in the preprocessor function (applys the function to each row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49999, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>i don t even know where to begin on this one i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>richard tyler is a little boy who is scared of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>i waited long to watch this movie also because...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "49996  i don t even know where to begin on this one i...          0\n",
       "49997  richard tyler is a little boy who is scared of...          0\n",
       "49998  i waited long to watch this movie also because...          1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# p.242 Processing documents into tokens\n",
    "# split the sentence/corpora into individual elements\n",
    "def tokenizer(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['running', 'like', 'running', 'and', 'thus', 'they', 'run']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('running like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word stemming, tranforming word into their root form\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_porter('running like running and thus they run')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/whs/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'run', 'lot']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')  # stop words have little meaning eg. a, is, and, has, etc. \n",
    "[w for w in tokenizer_porter('a runner likes running and runs a lot') \n",
    " if w not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2501,)\n"
     ]
    }
   ],
   "source": [
    "# pg. 244\n",
    "# Training a Logistic Regression model for document classification\n",
    "# (X,y)\n",
    "#X_train = df.loc[:25000, 'review'].values\n",
    "#y_train = df.loc[:25000, 'sentiment'].values\n",
    "\n",
    "#X_test  = df.loc[25000:, 'review'].values\n",
    "#y_test  = df.loc[25000:, 'sentiment'].values\n",
    "\n",
    "X_train = df.loc[:2500, 'review'].values\n",
    "y_train = df.loc[:2500, 'sentiment'].values\n",
    "\n",
    "X_test  = df.loc[2500:5000, 'review'].values\n",
    "y_test  = df.loc[2500:5000, 'sentiment'].values\n",
    "\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents = None, \n",
    "                       lowercase = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "              {'vect__ngram_range':[(1,1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer], #, tokenizer_porter],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [100]}, # 0.1, 1.0, 10.0, 100.0]},\n",
    "              \n",
    "              {'vect__ngram_range': [(1,1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer], #, tokenizer_porter],\n",
    "               'vect__use_idf':[False],\n",
    "               'vect__norm':[None],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C':[100]} #[0.1, 1.0,10.0,100.0]}\n",
    "                ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_tfidf = Pipeline([ ('vect', tfidf) ,\n",
    "                      ('clf',  LogisticRegression(random_state=0))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs_lr_tfidf = GridSearchCV( lr_tfidf, param_grid, #sends each subset to a different core\n",
    "                          scoring = 'accuracy',\n",
    "                          cv = 5, verbose = 1,\n",
    "                          n_jobs = -1) # n_jobs -1 uses all computer cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2501,) (2501,)\n",
      "(2501,) (2501,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   13.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...nalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid=[{'vect__ngram_range': [(1, 1)], 'vect__stop_words': [['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', '...0>], 'vect__use_idf': [False], 'vect__norm': [None], 'clf__penalty': ['l1', 'l2'], 'clf__C': [100]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Takes like 60 seconds\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Best parameter set: {'clf__C': 100, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x118d75730>}\n"
     ]
    }
   ],
   "source": [
    "print('The Best parameter set: %s' % gs_lr_tfidf.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy: 0.855\n",
      "Test Accuracy: 0.848\n"
     ]
    }
   ],
   "source": [
    "print('CV Accuracy: %.3f'\n",
    "     % gs_lr_tfidf.best_score_)\n",
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print('Test Accuracy: %.3f' % clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shiz getting real - Tweet Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'url_json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-e3047fd198c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdfJ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'url_json' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import urllib\n",
    "import urllib.request\n",
    "data = urllib.request.urlopen(url_json).read()\n",
    "output = json.loads(data)\n",
    "dfJ = pd.DataFrame(output)\n",
    "#print (output)\n",
    "dfJ.head()\n",
    "\n",
    "for date in dates:\n",
    "    date=date.strftime(\"%Y-%m-%d\")\n",
    "    date_json=date+'.json'\n",
    "    url='https://alexlitel.github.io/congresstweets/data/'\n",
    "    url_json=url+date_json\n",
    "    data = urllib.request.urlopen(url_json).read()\n",
    "    output = json.loads(data)\n",
    "    dfDate = pd.DataFrame(output)\n",
    "    predict=np.mean(clf.predict(dfDate['text']))\n",
    "    print('sucess'+date)\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####Predict Time \n",
    "#https://alexlitel.github.io/congresstweets/\n",
    "\n",
    "\n",
    "\n",
    "### Let's import our data\n",
    "df_20170629 = pd.read_json('2017-06-29.json')\n",
    "df_20170628 = pd.read_json('2017-06-28.json')\n",
    "df_20170627 = pd.read_json('2017-06-27.json')\n",
    "df_20170626 = pd.read_json('2017-06-26.json')\n",
    "df_20170625 = pd.read_json('2017-06-25.json')\n",
    "df_20170624 = pd.read_json('2017-06-24.json')\n",
    "df_20170623 = pd.read_json('2017-06-23.json')\n",
    "df_20170622 = pd.read_json('2017-06-22.json')\n",
    "\n",
    "#clf.predict(X_test[:10])\n",
    "#X_test[:10]\n",
    "predict_20170629=np.mean(clf.predict(df_20170629['text']))\n",
    "predict_20170628=np.mean(clf.predict(df_20170628['text']))\n",
    "predict_20170627=np.mean(clf.predict(df_20170627['text']))\n",
    "predict_20170626=np.mean(clf.predict(df_20170626['text']))\n",
    "predict_20170625=np.mean(clf.predict(df_20170625['text']))\n",
    "predict_20170624=np.mean(clf.predict(df_20170624['text']))\n",
    "predict_20170623=np.mean(clf.predict(df_20170623['text']))\n",
    "predict_20170622=np.mean(clf.predict(df_20170622['text']))\n",
    "\n",
    "df_plot=[predict_20170629, predict_20170628,\n",
    "         predict_20170627, predict_20170626,\n",
    "         predict_20170625,predict_20170624,\n",
    "         predict_20170623,predict_20170622]\n",
    "\n",
    "dates = pd.date_range('20170622', periods=8)\n",
    "\n",
    "\n",
    "dates\n",
    "df_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#set pictue\n",
    "#Picture Cedits Alina Oleynik\n",
    "from IPython.display import Image\n",
    "smiley=Image(\"Smiley.png\")\n",
    "frowney=Image(\"Frowney.png\")\n",
    "\n",
    "if predict_20170629 > .5:\n",
    "    facePic = smiley \n",
    "else:\n",
    "    facePic = frowney\n",
    "    \n",
    "facePic\n",
    "\n",
    "import urllib.request\n",
    "with urllib.request.urlopen('https://github.com/whs2k/whs2k.github.io/blob/master/Frowney.png?raw=true') as url:\n",
    "    facePic = url.read()\n",
    "#I'm guessing this would output the html source code?\n",
    "#print(s)\n",
    "outfile = open('facePic.png','wb')\n",
    "outfile.write(facePic)\n",
    "outfile.close()\n",
    "#facePic\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'linspace' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-433d44abd044>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mthfont\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'fontname'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'Tahoma'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_plot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'#daccc9'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'EMZ Estimates'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'linspace' is not defined"
     ]
    }
   ],
   "source": [
    "#X-Axis - Days\n",
    "#max_year=df['fiscal_year'].max()\n",
    "#min_year=df['fiscal_year'].min()\n",
    "#years=np.linspace(min_year, max_year, (max_year-min_year+1))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "thfont = {'fontname':'Tahoma'}\n",
    "x=linspace(1,6,7)\n",
    "\n",
    "plt.plot(dates, df_plot,'#daccc9', label='EMZ Estimates')\n",
    "plt.xlabel('Date',**thfont)\n",
    "plt.ylabel('Sentiment',**thfont)\n",
    "\n",
    "\n",
    "plt.savefig('todaysMood.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# STop Boi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#IMpotant Git Commands\n",
    "#git add "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from threading import Timer\n",
    "\n",
    "x=datetime.today()\n",
    "y=x.replace(day=x.day+1, hour=1, minute=0, second=0, microsecond=0)\n",
    "delta_t=y-x\n",
    "\n",
    "secs=delta_t.seconds+1\n",
    "\n",
    "def hello_world():\n",
    "    print (\"hello world\")\n",
    "    #...\n",
    "\n",
    "t = Timer(secs, hello_world)\n",
    "t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Skip this part of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# p.246\n",
    "# Working with bigger data -- online algos and out-of-core learning\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):  # converts reviews to lower case, take out non-words, and put emoticons at the end\n",
    "    text = re.sub('<[^>]*>', '', text) #\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "def stream_docs(path): #brings in the reviews\n",
    "    with open(path, 'r', encoding='utf-8')  as csv:\n",
    "        next(csv) # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2]) \n",
    "            yield text, label \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer #New trick with hashing vectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "vect = HashingVectorizer(decode_error = 'ignore',\n",
    "                         n_features = 2**21,\n",
    "                         preprocessor = None,\n",
    "                         tokenizer = tokenizer)\n",
    "\n",
    "clf = SGDClassifier(loss='log', random_state = 1, n_iter = 1)\n",
    "\n",
    "doc_stream = stream_docs(path ='/Users/whs/Documents/Grad/Machine Learning/Class 4/movie_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import pyprind\n",
    "classes = np.array([0, 1])\n",
    "print(classes)\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes = classes) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, size = 500)\n",
    "X_test = vect.transform(X_test)\n",
    "print('Accuracy: %.3f' % clf.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_20170628 = pd.read_json('2017-06-28.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Shiz getting real - Tweet Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from datetime import date, timedelta\n",
    "\n",
    "#Ceate a vaiarble: todays_tweets = \n",
    "#today=dt.datetime.today().strftime(\"%m/%d/%Y\")\n",
    "today = dt.date.today().strftime(\"%Y-%m-%d\")\n",
    "yesterday = dt.date.today() - timedelta(1)\n",
    "yesterday=yesterday.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://alexlitel.github.io/congresstweets/data/2017-07-04.json\n"
     ]
    }
   ],
   "source": [
    "#Ceate a vaiarble: todays_tweets = \n",
    "today=dt.datetime.today().strftime(\"%Y-%m-%d\")\n",
    "yesterday_json=yesterday+'.json'\n",
    "url='https://alexlitel.github.io/congresstweets/data/'\n",
    "url_json=url+yesterday_json\n",
    "#https://alexlitel.github.io/congresstweets/data/2017-07-04.json\n",
    "print(url_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-22 00:00:00\n",
      "2017-06-23 00:00:00\n",
      "2017-06-24 00:00:00\n",
      "2017-06-25 00:00:00\n",
      "2017-06-26 00:00:00\n",
      "2017-06-27 00:00:00\n",
      "2017-06-28 00:00:00\n",
      "2017-06-29 00:00:00\n",
      "2017-06-30 00:00:00\n",
      "2017-07-01 00:00:00\n",
      "2017-07-02 00:00:00\n",
      "2017-07-03 00:00:00\n",
      "2017-07-04 00:00:00\n",
      "DatetimeIndex(['2017-06-22', '2017-06-23', '2017-06-24', '2017-06-25',\n",
      "               '2017-06-26', '2017-06-27', '2017-06-28', '2017-06-29',\n",
      "               '2017-06-30', '2017-07-01', '2017-07-02', '2017-07-03',\n",
      "               '2017-07-04'],\n",
      "              dtype='datetime64[ns]', freq='D')\n"
     ]
    }
   ],
   "source": [
    "#Create Dates / A-axis\n",
    "from datetime import date\n",
    "d1 = date(2017, 6, 22)\n",
    "d0 = date.today()\n",
    "delta = d0 - d1\n",
    "periods=delta.days\n",
    "dates = pd.date_range('20170622', periods=periods)\n",
    "\n",
    "for date in dates:\n",
    "    print(date)\n",
    "print(dates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>882086618294755328</td>\n",
       "      <td>https://www.twitter.com/brianschatz/statuses/8...</td>\n",
       "      <td>brianschatz</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>This is worth your time. I promise. https://tw...</td>\n",
       "      <td>2017-07-04T00:00:12-04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>882096723123986436</td>\n",
       "      <td>https://www.twitter.com/TheNightGallery/status...</td>\n",
       "      <td>auctnr1</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @TheNightGallery Almost time for me to star...</td>\n",
       "      <td>2017-07-04T00:40:22-04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>882096666257629185</td>\n",
       "      <td>https://www.twitter.com/FSMidwest/statuses/882...</td>\n",
       "      <td>auctnr1</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @FSMidwest Luke Voit after his first Major ...</td>\n",
       "      <td>2017-07-04T00:40:08-04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>882096316117135360</td>\n",
       "      <td>https://www.twitter.com/RonHarrisMN/statuses/8...</td>\n",
       "      <td>Grace4NY</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @RonHarrisMN Our standards for our children...</td>\n",
       "      <td>2017-07-04T00:38:44-04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>882094376201011202</td>\n",
       "      <td>https://www.twitter.com/NateSilver538/statuses...</td>\n",
       "      <td>RepJayapal</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @NateSilver538 It's almost as though local ...</td>\n",
       "      <td>2017-07-04T00:31:02-04:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                                               link  \\\n",
       "0  882086618294755328  https://www.twitter.com/brianschatz/statuses/8...   \n",
       "1  882096723123986436  https://www.twitter.com/TheNightGallery/status...   \n",
       "2  882096666257629185  https://www.twitter.com/FSMidwest/statuses/882...   \n",
       "3  882096316117135360  https://www.twitter.com/RonHarrisMN/statuses/8...   \n",
       "4  882094376201011202  https://www.twitter.com/NateSilver538/statuses...   \n",
       "\n",
       "   screen_name              source  \\\n",
       "0  brianschatz  Twitter for iPhone   \n",
       "1      auctnr1  Twitter for iPhone   \n",
       "2      auctnr1  Twitter for iPhone   \n",
       "3     Grace4NY  Twitter for iPhone   \n",
       "4   RepJayapal  Twitter for iPhone   \n",
       "\n",
       "                                                text  \\\n",
       "0  This is worth your time. I promise. https://tw...   \n",
       "1  RT @TheNightGallery Almost time for me to star...   \n",
       "2  RT @FSMidwest Luke Voit after his first Major ...   \n",
       "3  RT @RonHarrisMN Our standards for our children...   \n",
       "4  RT @NateSilver538 It's almost as though local ...   \n",
       "\n",
       "                        time  \n",
       "0  2017-07-04T00:00:12-04:00  \n",
       "1  2017-07-04T00:40:22-04:00  \n",
       "2  2017-07-04T00:40:08-04:00  \n",
       "3  2017-07-04T00:38:44-04:00  \n",
       "4  2017-07-04T00:31:02-04:00  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfJson.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    And that's a wrap, folks! #FY18NDAA is approve...\n",
       "1    #FY18NDAA passes out of committee at 11:58 pm ...\n",
       "2    RT @reporterjoe HASC overwhelmingly passes 201...\n",
       "3    This fight to defeat the GOP “health care” pla...\n",
       "4    @HASCRepublicans Proud many priorities for #IN...\n",
       "5    Just finished marking up the NDAA in record ti...\n",
       "6    .@realDonaldTrump lost the popular vote in Ame...\n",
       "7    Very proud to be part of Congress that is focu...\n",
       "8    RT @davepell Don’t believe the fear mongers. D...\n",
       "9    Pleased to see @realDonaldTrump lawyers conced...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test Before Loop\n",
    "#predict=np.mean(clf.predict(dfJson['text']))\n",
    "import json\n",
    "import urllib\n",
    "import urllib.request\n",
    "data = urllib.request.urlopen(url_json).read()\n",
    "output = json.loads(data)\n",
    "dfJson = pd.DataFrame(output)\n",
    "#print (output)\n",
    "dfJson.head()\n",
    "dfJson['text'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Cread new pivot dfPlot\n",
    "dfPlot = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yesterday_json=yesterday+'.json'\n",
    "url='https://alexlitel.github.io/congresstweets/data/'\n",
    "url_json=url+yesterday_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dates=dates.strftime(\"%Y-%m-%d\")\n",
    "date_json=date+'.json'\n",
    "url='https://alexlitel.github.io/congresstweets/data/'\n",
    "url_json=url+date_json\n",
    "data = urllib.request.urlopen(url_json).read()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate a non-NDFrame object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-18835bad70c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mdfJson\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfJson\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mdfPlot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;31m#dfPlot[date] = predict#add to dfPlot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m#dfPlot.set_value(1, dfPlot[date], 'date')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, ignore_index, verify_integrity)\u001b[0m\n\u001b[1;32m   4545\u001b[0m             \u001b[0mto_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4546\u001b[0m         return concat(to_concat, ignore_index=ignore_index,\n\u001b[0;32m-> 4547\u001b[0;31m                       verify_integrity=verify_integrity)\n\u001b[0m\u001b[1;32m   4548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4549\u001b[0m     def join(self, other, on=None, how='left', lsuffix='', rsuffix='',\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\u001b[0m\n\u001b[1;32m    204\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                        copy=copy)\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobjs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNDFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot concatenate a non-NDFrame object\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0;31m# consolidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot concatenate a non-NDFrame object"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import urllib\n",
    "import urllib.request\n",
    "data = urllib.request.urlopen(url_json).read()\n",
    "output = json.loads(data)\n",
    "dfJson = pd.DataFrame(output)\n",
    "#print (output)\n",
    "dfJson.head()\n",
    "\n",
    "for date in dates:\n",
    "    #dates=dates.strftime(\"%Y-%m-%d\")\n",
    "    date_json=date+'.json'\n",
    "    url='https://alexlitel.github.io/congresstweets/data/'\n",
    "    url_json=url+date_json\n",
    "    data = urllib.request.urlopen(url_json).read()\n",
    "    output = json.loads(data)\n",
    "    dfJson = pd.DataFrame(output)\n",
    "    predict = np.mean(clf.predict(dfJson['text']))\n",
    "    dfPlot.append(predict)\n",
    "    #dfPlot[date] = predict#add to dfPlot\n",
    "    #dfPlot.set_value(1, dfPlot[date], 'date')\n",
    "    \n",
    "    print('sucess'+date)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.49258908861557871,\n",
       " 0.49258908861557871,\n",
       " 0.49258908861557871,\n",
       " 0.49258908861557871,\n",
       " 0.49258908861557871,\n",
       " 0.49258908861557871,\n",
       " 0.49258908861557871,\n",
       " 0.49258908861557871,\n",
       " 0.49258908861557871,\n",
       " 0.49258908861557871,\n",
       " 0.49258908861557871,\n",
       " 0.49258908861557871,\n",
       " 0.49258908861557871]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfPlot\n",
    "#del dfPlot[:]\n",
    "#dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4967698061883713,\n",
       " 0.47105357784969437,\n",
       " 0.48583523282318464,\n",
       " 0.47185493010955798,\n",
       " 0.48214285714285715,\n",
       " 0.59193954659949621,\n",
       " 0.49723756906077349,\n",
       " 0.49258908861557871]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####Predict Time \n",
    "#https://alexlitel.github.io/congresstweets/\n",
    "\n",
    "\n",
    "\n",
    "### Let's import our data\n",
    "df_20170629 = pd.read_json('2017-06-29.json')\n",
    "df_20170628 = pd.read_json('2017-06-28.json')\n",
    "df_20170627 = pd.read_json('2017-06-27.json')\n",
    "df_20170626 = pd.read_json('2017-06-26.json')\n",
    "df_20170625 = pd.read_json('2017-06-25.json')\n",
    "df_20170624 = pd.read_json('2017-06-24.json')\n",
    "df_20170623 = pd.read_json('2017-06-23.json')\n",
    "df_20170622 = pd.read_json('2017-06-22.json')\n",
    "\n",
    "#clf.predict(X_test[:10])\n",
    "#X_test[:10]\n",
    "predict_20170629=np.mean(clf.predict(df_20170629['text']))\n",
    "predict_20170628=np.mean(clf.predict(df_20170628['text']))\n",
    "predict_20170627=np.mean(clf.predict(df_20170627['text']))\n",
    "predict_20170626=np.mean(clf.predict(df_20170626['text']))\n",
    "predict_20170625=np.mean(clf.predict(df_20170625['text']))\n",
    "predict_20170624=np.mean(clf.predict(df_20170624['text']))\n",
    "predict_20170623=np.mean(clf.predict(df_20170623['text']))\n",
    "predict_20170622=np.mean(clf.predict(df_20170622['text']))\n",
    "\n",
    "df_plot=[predict_20170629, predict_20170628,\n",
    "         predict_20170627, predict_20170626,\n",
    "         predict_20170625,predict_20170624,\n",
    "         predict_20170623,predict_20170622]\n",
    "\n",
    "dates = pd.date_range('20170622', periods=8)\n",
    "\n",
    "\n",
    "dates\n",
    "df_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
