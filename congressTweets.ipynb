{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download data from site -- http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "# p.234\n",
    "#import pyprind # may need to do >sudo easy_install pip, then >pip install pyprind --user\n",
    "import pandas as pd\n",
    "import os\n",
    "#import pyprind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/whs/Documents/DataJournalism/CongressionalTweets/whs2k.github.io\n"
     ]
    }
   ],
   "source": [
    "pwd = os.getcwd()\n",
    "print(pwd)\n",
    "file = os.listdir(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49999, 2)\n"
     ]
    }
   ],
   "source": [
    "#################\n",
    "# Start here    #\n",
    "#################\n",
    "#import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "pwd = os.getcwd()\n",
    "\n",
    "df = pd.read_csv(pwd+'/movie_data.csv', encoding='utf-8')\n",
    "df.columns = ['review', 'sentiment']\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  I recently bought the DVD, forgetting just how...          0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 5, 'sun': 3, 'is': 1, 'shining': 2, 'weather': 6, 'sweet': 4, 'and': 0}\n",
      "[[0 1 1 1 0 1 0]\n",
      " [0 1 0 0 1 1 1]\n",
      " [1 2 1 1 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "# Bag of Word model\n",
    "# 1. create a vocabulary of unique tokens (or words)\n",
    "# 2. construct a feature vector for each document, features store count\n",
    "#    of words per document\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer() #Instanstiate the count array\n",
    "\n",
    "docs = np.array(['The sun is shining', \n",
    "                 'The weather is sweet',\n",
    "                 'The sun is shining and the weather is sweet'])\n",
    "\n",
    "bag = count.fit_transform(docs)\n",
    "\n",
    "print(count.vocabulary_)\n",
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf(t,d) - raw term frequencies (t: term, d: nos times term t appears in doc d)\n",
    "# tf-idf(t,d) - term frequency inverse document frequency\n",
    "# tf-idf = tf(t,d) * idf(t,x)  = tf(t,d) * log( [1+nd]/[1+df(d,t)] ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.    0.43  0.56  0.56  0.    0.43  0.  ]\n",
      " [ 0.    0.43  0.    0.    0.56  0.43  0.56]\n",
      " [ 0.4   0.48  0.31  0.31  0.31  0.48  0.31]]\n"
     ]
    }
   ],
   "source": [
    "# TfidTransformer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer() #Instantiate Term Frequency invers\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray()) #How much did the term appear in other documents?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# so if the term \"the\" shows up lots of times, does that mean it's important?\n",
    "# how can we make terms that shows up lots of times across documents, less important\n",
    "# let's normalize by the times these terms show up across documents.\n",
    "\n",
    "# employ : [nos of docs containing term \"the\" ]/[total nos of documents]\n",
    "\n",
    "# if term appear often, give it less emphasis\n",
    "\n",
    "# tf-idf(t,d) = tf(t,2)*(idf(t,d)+1)\n",
    "# with idf(t,d) = log ([1+total nos of docs]/[1+nos of docs containing term t])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reg functions...to get rid of HTML Tags and emoticons\n",
    "\n",
    "import re\n",
    "def preprocessor(text): \n",
    "# find '<' then anything not '>' [^>], [^>]* 0 or more prefix, then close with '>'    \n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text) \n",
    "    # eyes[:,;,=], optional nose [-], and mouth[),(,D,P)]\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is seven title brazil not available;) :)\n"
     ]
    }
   ],
   "source": [
    "tmp = 'is ;) :) seven.<br /><br />Title (Brazil): Not Available'\n",
    "\n",
    "print(preprocessor(tmp))\n",
    "#print(preprocessor('</a>This :) is :( a test :-)!' ))\n",
    "#print(re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', '</a>This :) is :( a test :-)!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(preprocessor) #use the apply method and send in the preprocessor function (applys the function to each row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49999, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>i don t even know where to begin on this one i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>richard tyler is a little boy who is scared of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>i waited long to watch this movie also because...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "49996  i don t even know where to begin on this one i...          0\n",
       "49997  richard tyler is a little boy who is scared of...          0\n",
       "49998  i waited long to watch this movie also because...          1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# p.242 Processing documents into tokens\n",
    "# split the sentence/corpora into individual elements\n",
    "def tokenizer(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['running', 'like', 'running', 'and', 'thus', 'they', 'run']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('running like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word stemming, tranforming word into their root form\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_porter('running like running and thus they run')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/whs/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'run', 'lot']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')  # stop words have little meaning eg. a, is, and, has, etc. \n",
    "[w for w in tokenizer_porter('a runner likes running and runs a lot') \n",
    " if w not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2501,)\n"
     ]
    }
   ],
   "source": [
    "# pg. 244\n",
    "# Training a Logistic Regression model for document classification\n",
    "# (X,y)\n",
    "#X_train = df.loc[:25000, 'review'].values\n",
    "#y_train = df.loc[:25000, 'sentiment'].values\n",
    "\n",
    "#X_test  = df.loc[25000:, 'review'].values\n",
    "#y_test  = df.loc[25000:, 'sentiment'].values\n",
    "\n",
    "X_train = df.loc[:2500, 'review'].values\n",
    "y_train = df.loc[:2500, 'sentiment'].values\n",
    "\n",
    "X_test  = df.loc[2500:5000, 'review'].values\n",
    "y_test  = df.loc[2500:5000, 'sentiment'].values\n",
    "\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents = None, \n",
    "                       lowercase = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "              {'vect__ngram_range':[(1,1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer], #, tokenizer_porter],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [100]}, # 0.1, 1.0, 10.0, 100.0]},\n",
    "              \n",
    "              {'vect__ngram_range': [(1,1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer], #, tokenizer_porter],\n",
    "               'vect__use_idf':[False],\n",
    "               'vect__norm':[None],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C':[100]} #[0.1, 1.0,10.0,100.0]}\n",
    "                ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_tfidf = Pipeline([ ('vect', tfidf) ,\n",
    "                      ('clf',  LogisticRegression(random_state=0))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs_lr_tfidf = GridSearchCV( lr_tfidf, param_grid, #sends each subset to a different core\n",
    "                          scoring = 'accuracy',\n",
    "                          cv = 5, verbose = 1,\n",
    "                          n_jobs = -1) # n_jobs -1 uses all computer cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2501,) (2501,)\n",
      "(2501,) (2501,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   20.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...nalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid=[{'vect__ngram_range': [(1, 1)], 'vect__stop_words': [['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', '...8>], 'vect__use_idf': [False], 'vect__norm': [None], 'clf__penalty': ['l1', 'l2'], 'clf__C': [100]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Takes like 60 seconds\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Best parameter set: {'clf__C': 100, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x11bf3f9d8>}\n"
     ]
    }
   ],
   "source": [
    "print('The Best parameter set: %s' % gs_lr_tfidf.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy: 0.855\n",
      "Test Accuracy: 0.848\n"
     ]
    }
   ],
   "source": [
    "print('CV Accuracy: %.3f'\n",
    "     % gs_lr_tfidf.best_score_)\n",
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print('Test Accuracy: %.3f' % clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_20170628 = pd.read_json('2017-06-28.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Shiz getting real - Tweet Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from datetime import date, timedelta\n",
    "\n",
    "#Ceate a vaiarble: todays_tweets = \n",
    "#today=dt.datetime.today().strftime(\"%m/%d/%Y\")\n",
    "today = dt.date.today().strftime(\"%Y-%m-%d\")\n",
    "yesterday = dt.date.today() - timedelta(1)\n",
    "yesterday=yesterday.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://alexlitel.github.io/congresstweets/data/2017-07-09.json\n"
     ]
    }
   ],
   "source": [
    "#Ceate a vaiarble: todays_tweets = \n",
    "today=dt.datetime.today().strftime(\"%Y-%m-%d\")\n",
    "yesterday_json=yesterday+'.json'\n",
    "url='https://alexlitel.github.io/congresstweets/data/'\n",
    "url_json=url+yesterday_json\n",
    "#https://alexlitel.github.io/congresstweets/data/2017-07-04.json\n",
    "print(url_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-22 00:00:00\n",
      "2017-06-23 00:00:00\n",
      "2017-06-24 00:00:00\n",
      "2017-06-25 00:00:00\n",
      "2017-06-26 00:00:00\n",
      "2017-06-27 00:00:00\n",
      "2017-06-28 00:00:00\n",
      "2017-06-29 00:00:00\n",
      "2017-06-30 00:00:00\n",
      "2017-07-01 00:00:00\n",
      "2017-07-02 00:00:00\n",
      "2017-07-03 00:00:00\n",
      "2017-07-04 00:00:00\n",
      "2017-07-05 00:00:00\n",
      "2017-07-06 00:00:00\n",
      "2017-07-07 00:00:00\n",
      "2017-07-08 00:00:00\n",
      "2017-07-09 00:00:00\n",
      "DatetimeIndex(['2017-06-22', '2017-06-23', '2017-06-24', '2017-06-25',\n",
      "               '2017-06-26', '2017-06-27', '2017-06-28', '2017-06-29',\n",
      "               '2017-06-30', '2017-07-01', '2017-07-02', '2017-07-03',\n",
      "               '2017-07-04', '2017-07-05', '2017-07-06', '2017-07-07',\n",
      "               '2017-07-08', '2017-07-09'],\n",
      "              dtype='datetime64[ns]', freq='D')\n"
     ]
    }
   ],
   "source": [
    "#Create Dates / A-axis\n",
    "from datetime import date\n",
    "d1 = date(2017, 6, 22)\n",
    "d0 = date.today()\n",
    "delta = d0 - d1\n",
    "periods=delta.days\n",
    "dates = pd.date_range('20170622', periods=periods)\n",
    "\n",
    "for date in dates:\n",
    "    print(date)\n",
    "print(dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Change Dates to string fo manipulation\n",
    "datesStr=dates.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cread new pivot dfPlot\n",
    "dfPlot = pd.DataFrame()\n",
    "dfPlot['Score']=0\n",
    "#dfPlot['Date']=dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib\n",
    "import urllib.request\n",
    "data = urllib.request.urlopen(url_json).read()\n",
    "output = json.loads(data)\n",
    "dfJson = pd.DataFrame(output)\n",
    "#print (output)\n",
    "dfJson.head()\n",
    "\n",
    "for date in datesStr:\n",
    "    #dates=dates.strftime(\"%Y-%m-%d\")\n",
    "    date_json=date+'.json'\n",
    "    url='https://alexlitel.github.io/congresstweets/data/'\n",
    "    url_json=url+date_json\n",
    "    data = urllib.request.urlopen(url_json).read()\n",
    "    output = json.loads(data)\n",
    "    dfJson = pd.DataFrame(output)\n",
    "    predict = np.mean(clf.predict(dfJson['text']))\n",
    "    #dfPlot=dfPlot.append(predict)\n",
    "    dfPlot.loc[date]=predict\n",
    "    #dfPlot[date] = predict#add to dfPlot\n",
    "    #dfPlot.set_value(1, dfPlot[date], 'date')\n",
    "    print('sucess'+date)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPlot.head()\n",
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set pictue\n",
    "#Picture Cedits Alina Oleynik\n",
    "from IPython.display import Image\n",
    "smiley=Image(\"Smiley.png\")\n",
    "frowney=Image(\"Frowney.png\")\n",
    "\n",
    "if predict > .5:\n",
    "    facePic = smiley \n",
    "else:\n",
    "    facePic = frowney\n",
    "    \n",
    "facePic\n",
    "\n",
    "import urllib.request\n",
    "with urllib.request.urlopen('https://github.com/whs2k/whs2k.github.io/blob/master/Frowney.png?raw=true') as url:\n",
    "    facePic = url.read()\n",
    "#I'm guessing this would output the html source code?\n",
    "#print(s)\n",
    "outfile = open('facePic.png','wb')\n",
    "outfile.write(facePic)\n",
    "outfile.close()\n",
    "facePic\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X-Axis - Days\n",
    "#max_year=df['fiscal_year'].max()\n",
    "#min_year=df['fiscal_year'].min()\n",
    "#years=np.linspace(min_year, max_year, (max_year-min_year+1))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.cbook as cbook\n",
    "thfont = {'fontname':'Tahoma'}\n",
    "\n",
    "#https://matplotlib.org/users/recipes.html\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.plot(dates, dfPlot['Score'])\n",
    "\n",
    "# rotate and align the tick labels so they look better\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "# use a more precise date string for the x axis locations in the\n",
    "# toolbar\n",
    "import matplotlib.dates as mdates\n",
    "ax.fmt_xdata = mdates.DateFormatter('%Y-%m-%d')\n",
    "plt.title('Congress Mood By Twitter')\n",
    "\n",
    "plt.xlabel('Date',**thfont)\n",
    "plt.ylabel('Sentiment',**thfont)\n",
    "plt.savefig(pwd+'/todaysMood.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepping and Automating this Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert this notebook to a script\n",
    "#$ ipython nbconvert --to script \"congressTweets.ipynb\"\n",
    "\n",
    "#Then Execute\n",
    "#$ python \"congressTweets.py\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Automate it\n",
    "#http://naelshiab.com/tutorial-how-to-automatically-run-your-scripts-on-your-computer/\n",
    "#1. Create a new text file: \n",
    "#    #!/bin/sh\n",
    "#    python python /Users/whs/Documents/DataJournalism/CongressionalTweets/whs2k.github.io/congressTwitter.py\n",
    "#2. Save it with no extension\n",
    "#3. Convert it to an excecutable \n",
    "#    chmod 755 command\n",
    "#4. Set it up as an app in automater\n",
    "#5. Make it an alert on calender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Automate a git file\n",
    "#Last you need to git\n",
    "#git add ..\n",
    "#git commit -m \"daily update\"\n",
    "#git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make A Cool Word Cloud\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from os import path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "#d = path.dirname(twitter_cloud)\n",
    "\n",
    "# Read the whole text.\n",
    "#text = open(path.join(d, 'alice.txt')).read()\n",
    "text = dfJson['text']\n",
    "\n",
    "# read the mask image\n",
    "# taken from\n",
    "# http://www.stencilry.org/stencils/movies/alice%20in%20wonderland/255fk.jpg\n",
    "twitter_mask = np.array(Image.open(\"twitter.png\"))\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "#stopwords.add(\"said\")\n",
    "\n",
    "wc = WordCloud(background_color=\"white\", max_words=2000, mask=twitter_mask,\n",
    "               stopwords=stopwords)\n",
    "# generate word cloud\n",
    "wc.generate(text)\n",
    "\n",
    "# store to file\n",
    "wc.to_file(path.join(d, \"twitter_cloud.png\"))\n",
    "\n",
    "# show\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.figure()\n",
    "plt.imshow(alice_mask, cmap=plt.cm.gray, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
